{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarjit420/Assistant_APIs/blob/main/Assistants_api_overview_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3mE7ANPLErx"
      },
      "source": [
        "# Assistants API Overview (Python SDK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deDYU6QSLEr0"
      },
      "source": [
        "The new [Assistants API](https://platform.openai.com/docs/assistants/overview) is a stateful evolution of our [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) meant to simplify the creation of assistant-like experiences, and enable developer access to powerful tools like Code Interpreter and Retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfg-JtsTLEr1"
      },
      "source": [
        "![Assistants API Diagram](https://github.com/openai/openai-cookbook/blob/main/images/assistants_overview_diagram.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVTxHbUTLEr1"
      },
      "source": [
        "## Chat Completions API vs Assistants API\n",
        "\n",
        "The primitives of the **Chat Completions API** are `Messages`, on which you perform a `Completion` with a `Model` (`gpt-3.5-turbo`, `gpt-4`, etc). It is lightweight and powerful, but inherently stateless, which means you have to manage conversation state, tool definitions, retrieval documents, and code execution manually.\n",
        "\n",
        "The primitives of the **Assistants API** are\n",
        "\n",
        "- `Assistants`, which encapsulate a base model, instructions, tools, and (context) documents,\n",
        "- `Threads`, which represent the state of a conversation, and\n",
        "- `Runs`, which power the execution of an `Assistant` on a `Thread`, including textual responses and multi-step tool use.\n",
        "\n",
        "We'll take a look at how these can be used to create powerful, stateful experiences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LesTyhriLEr2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Python SDK\n",
        "\n",
        "> **Note**\n",
        "> We've updated our [Python SDK](https://github.com/openai/openai-python) to add support for the Assistants API, so you'll need to update it to the latest version (`1.2.3` at time of writing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs65dS49LEr2",
        "outputId": "e1ea020e-e47a-434d-dfe7-54db6b6336f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.4)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uHNk4vfLEr3"
      },
      "source": [
        "And make sure it's up to date by running:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjQfgBfJLEr3",
        "outputId": "e40926cc-0d10-4f89-f790-975ae36dfd6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 1.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip show openai | grep Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwTwbmaPLEr4"
      },
      "source": [
        "### Pretty Printing Helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoOkyAYWLEr4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def show_json(obj):\n",
        "    display(json.loads(obj.model_dump_json()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLfOlOUkLEr4"
      },
      "source": [
        "## Complete Example with Assistants API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cworLozFLEr4"
      },
      "source": [
        "### Assistants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEKqsJUeLEr4"
      },
      "source": [
        "The easiest way to get started with the Assistants API is through the [Assistants Playground](https://platform.openai.com/playground).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNFSAdfaLEr4"
      },
      "source": [
        "![Assistants Playground](https://github.com/openai/openai-cookbook/blob/main/images/assistants_overview_assistants_playground.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRt2YfYlLEr5"
      },
      "source": [
        "Let's begin by creating an assistant! We'll create a Math Tutor just like in our [docs](https://platform.openai.com/docs/assistants/overview).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEeU_svwLEr5"
      },
      "source": [
        "![Creating New Assistant](https://github.com/openai/openai-cookbook/blob/main/images/assistants_overview_new_assistant.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdsL10qdLEr5"
      },
      "source": [
        "You can view Assistants you've created in the [Assistants Dashboard](https://platform.openai.com/assistants).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdBfYwOELEr5"
      },
      "source": [
        "![Assistants Dashboard](https://github.com/openai/openai-cookbook/blob/main/images/assistants_overview_assistants_dashboard.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm3PZRS0LEr5"
      },
      "source": [
        "You can also create Assistants directly through the Assistants API, like so:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9CEu9KOLEr5",
        "outputId": "50e3e6c0-821c-4163-9c49-9f97de934748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'asst_B0qsE2jkbi2QRO5PTrt25euQ',\n",
              " 'created_at': 1700655709,\n",
              " 'description': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo-1106',\n",
              " 'name': 'Math Tutor',\n",
              " 'object': 'assistant',\n",
              " 'tools': []}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=\"???\")\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Math Tutor\",\n",
        "    instructions=\"You are a personal math tutor. Answer questions briefly, in a sentence or less.\",\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        ")\n",
        "show_json(assistant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daV_XNg9LEr5"
      },
      "source": [
        "Regardless of whether you create your Assistant through the Dashboard or with the API, you'll want to keep track of the Assistant ID. This is how you'll refer to your Assistant throughout Threads and Runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0FOkCe4LEr5"
      },
      "source": [
        "Next, we'll create a new Thread and add a Message to it. This will hold the state of our conversation, so we don't have re-send the entire message history each time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDFLx0maLEr5"
      },
      "source": [
        "### Threads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0RJGNmDLEr5"
      },
      "source": [
        "Create a new thread:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJYkHFd5LEr5",
        "outputId": "39159421-70db-4e0a-e736-828fca2ce564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'thread_EHFo3CQoM1qReLmjc7trk2MI',\n",
              " 'created_at': 1700658544,\n",
              " 'metadata': {},\n",
              " 'object': 'thread'}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "thread = client.beta.threads.create()\n",
        "show_json(thread)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyac5JTYLEr5"
      },
      "source": [
        "Then add the Message to the thread:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6XgfF99LEr5",
        "outputId": "6ae382a7-6169-49ef-8c3b-0b5ff159b903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'msg_YJX2LXCoPxQGewlbBKL5WYCP',\n",
              " 'assistant_id': None,\n",
              " 'content': [{'text': {'annotations': [],\n",
              "    'value': 'I need to solve the equation `3x + 11 = 14`. Can you help me?'},\n",
              "   'type': 'text'}],\n",
              " 'created_at': 1700658547,\n",
              " 'file_ids': [],\n",
              " 'metadata': {},\n",
              " 'object': 'thread.message',\n",
              " 'role': 'user',\n",
              " 'run_id': None,\n",
              " 'thread_id': 'thread_EHFo3CQoM1qReLmjc7trk2MI'}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\",\n",
        ")\n",
        "show_json(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U53s9A-LEr5"
      },
      "source": [
        "> **Note**\n",
        "> Even though you're no longer sending the entire history each time, you will still be charged for the tokens of the entire conversation history with each Run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz4_n0dpLEr6"
      },
      "source": [
        "### Runs\n",
        "\n",
        "Notice how the Thread we created is **not** associated with the Assistant we created earlier! Threads exist independently from Assistants, which may be different from what you'd expect if you've used ChatGPT (where a thread is tied to a model/GPT).\n",
        "\n",
        "To get a completion from an Assistant for a given Thread, we must create a Run. Creating a Run will indicate to an Assistant it should look at the messages in the Thread and take action: either by adding a single response, or using tools.\n",
        "\n",
        "> **Note**\n",
        "> Runs are a key difference between the Assistants API and Chat Completions API. While in Chat Completions the model will only ever respond with a single message, in the Assistants API a Run may result in an Assistant using one or multiple tools, and potentially adding multiple messages to the Thread.\n",
        "\n",
        "To get our Assistant to respond to the user, let's create the Run. As mentioned earlier, you must specify _both_ the Assistant and the Thread.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x8d1wVYLEr6",
        "outputId": "c17aa72a-58af-4eac-9eab-075d511d278b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 'run_8GcRM2CBc28hx2iJovmCqPFW',\n",
              " 'assistant_id': 'asst_B0qsE2jkbi2QRO5PTrt25euQ',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': None,\n",
              " 'created_at': 1700658566,\n",
              " 'expires_at': 1700659166,\n",
              " 'failed_at': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'last_error': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-3.5-turbo-1106',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': None,\n",
              " 'started_at': None,\n",
              " 'status': 'queued',\n",
              " 'thread_id': 'thread_EHFo3CQoM1qReLmjc7trk2MI',\n",
              " 'tools': []}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")\n",
        "show_json(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZhoojLkLEr6"
      },
      "source": [
        "Unlike creating a completion in the Chat Completions API, **creating a Run is an asynchronous operation**. It will return immediately with the Run's metadata, which includes a `status` that will initially be set to `queued`. The `status` will be updated as the Assistant performs operations (like using tools and adding messages).\n",
        "\n",
        "To know when the Assistant has completed processing, we can poll the Run in a loop. (Support for streaming is coming soon!) While here we are only checking for a `queued` or `in_progress` status, in practice a Run may undergo a [variety of status changes](https://platform.openai.com/docs/api-reference/runs/object#runs/object-status) which you can choose to surface to the user. (These are called Steps, and will be covered later.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dHa1VHALEr6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_on_run(run, thread):\n",
        "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "        )\n",
        "        time.sleep(0.5)\n",
        "    return run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzm186OILEr6",
        "outputId": "56a5b84c-9757-4bd7-fd90-6672a1df4afd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'run_LA08RjouV3RemQ78UZXuyzv6',\n",
              " 'assistant_id': 'asst_9HAjl9y41ufsViNcThW1EXUS',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': 1699828333,\n",
              " 'created_at': 1699828332,\n",
              " 'expires_at': None,\n",
              " 'failed_at': None,\n",
              " 'file_ids': [],\n",
              " 'instructions': 'You are a personal math tutor. Answer questions briefly, in a sentence or less.',\n",
              " 'last_error': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-4-1106-preview',\n",
              " 'object': 'thread.run',\n",
              " 'required_action': None,\n",
              " 'started_at': 1699828332,\n",
              " 'status': 'completed',\n",
              " 'thread_id': 'thread_bw42vPoQtYBMQE84WubNcJXG',\n",
              " 'tools': []}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wait_on_run(run, thread)\n",
        "show_json(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eMppVVyLEr6"
      },
      "source": [
        "### Messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kjYkvD_LEr6"
      },
      "source": [
        "Now that the Run has completed, we can list the Messages in the Thread to see what got added by the Assistant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7AKLMS_LEr6",
        "outputId": "7ab88cc5-cfd7-43f1-f5da-b53ad721ecf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'data': [{'id': 'msg_r7RJMhmsYD6aNAYEtALZlKwR',\n",
              "   'assistant_id': 'asst_B0qsE2jkbi2QRO5PTrt25euQ',\n",
              "   'content': [{'text': {'annotations': [],\n",
              "      'value': 'Sure! Subtract 11 from both sides to isolate the variable.'},\n",
              "     'type': 'text'}],\n",
              "   'created_at': 1700658570,\n",
              "   'file_ids': [],\n",
              "   'metadata': {},\n",
              "   'object': 'thread.message',\n",
              "   'role': 'assistant',\n",
              "   'run_id': 'run_8GcRM2CBc28hx2iJovmCqPFW',\n",
              "   'thread_id': 'thread_EHFo3CQoM1qReLmjc7trk2MI'},\n",
              "  {'id': 'msg_YJX2LXCoPxQGewlbBKL5WYCP',\n",
              "   'assistant_id': None,\n",
              "   'content': [{'text': {'annotations': [],\n",
              "      'value': 'I need to solve the equation `3x + 11 = 14`. Can you help me?'},\n",
              "     'type': 'text'}],\n",
              "   'created_at': 1700658547,\n",
              "   'file_ids': [],\n",
              "   'metadata': {},\n",
              "   'object': 'thread.message',\n",
              "   'role': 'user',\n",
              "   'run_id': None,\n",
              "   'thread_id': 'thread_EHFo3CQoM1qReLmjc7trk2MI'}],\n",
              " 'object': 'list',\n",
              " 'first_id': 'msg_r7RJMhmsYD6aNAYEtALZlKwR',\n",
              " 'last_id': 'msg_YJX2LXCoPxQGewlbBKL5WYCP',\n",
              " 'has_more': False}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "show_json(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK4DDEvELEr6"
      },
      "source": [
        "As you can see, Messages are ordered in reverse-chronological order â€“ this was done so the most recent results are always on the first `page` (since results can be paginated). Do keep a look out for this, since this is the opposite order to messages in the Chat Completions API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7a5tlroLEr7"
      },
      "source": [
        "Let's ask our Assistant to explain the result a bit further!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1353_zTwLEr7",
        "outputId": "c67651b2-054e-45f0-8617-3f57a45e94eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'data': [{'id': 'msg_ziZEEclsWNQEF8KWa7q4rEif',\n",
              "   'assistant_id': 'asst_B0qsE2jkbi2QRO5PTrt25euQ',\n",
              "   'content': [{'text': {'annotations': [],\n",
              "      'value': \"Of course! When you subtract 11 from both sides, you're left with 3x = 3. Then, divide both sides by 3 to solve for x.\"},\n",
              "     'type': 'text'}],\n",
              "   'created_at': 1700658605,\n",
              "   'file_ids': [],\n",
              "   'metadata': {},\n",
              "   'object': 'thread.message',\n",
              "   'role': 'assistant',\n",
              "   'run_id': 'run_iNeheLn8Laya0GXOFB9uVFrL',\n",
              "   'thread_id': 'thread_EHFo3CQoM1qReLmjc7trk2MI'}],\n",
              " 'object': 'list',\n",
              " 'first_id': 'msg_ziZEEclsWNQEF8KWa7q4rEif',\n",
              " 'last_id': 'msg_ziZEEclsWNQEF8KWa7q4rEif',\n",
              " 'has_more': False}"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a message to append to our thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id, role=\"user\", content=\"Could you explain this to me?\"\n",
        ")\n",
        "\n",
        "# Execute our run\n",
        "run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "# Wait for completion\n",
        "wait_on_run(run, thread)\n",
        "\n",
        "# Retrieve all the messages added after our last user message\n",
        "messages = client.beta.threads.messages.list(\n",
        "    thread_id=thread.id, order=\"asc\", after=message.id\n",
        ")\n",
        "show_json(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVQNjl-cLEr_"
      },
      "source": [
        "This may feel like a lot of steps to get a response back, especially for this simple example. However, you'll soon see how we can add very powerful functionality to our Assistant without changing much code at all!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGd-VvlOLEr_"
      },
      "source": [
        "### Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCuXPqYrLEr_"
      },
      "source": [
        "Let's take a look at how we could potentially put all of this together. Below is all the code you need to use an Assistant you've created.\n",
        "\n",
        "Since we've already created our Math Assistant, I've saved its ID in `MATH_ASSISTANT_ID`. I then defined two functions:\n",
        "\n",
        "- `submit_message`: create a Message on a Thread, then start (and return) a new Run\n",
        "- `get_response`: returns the list of Messages in a Thread\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fndYInFhLEr_"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "MATH_ASSISTANT_ID = assistant.id  # or a hard-coded ID like \"asst-...\"\n",
        "\n",
        "def submit_message(assistant_id, thread, user_message):\n",
        "    client.beta.threads.messages.create(\n",
        "        thread_id=thread.id, role=\"user\", content=user_message\n",
        "    )\n",
        "    return client.beta.threads.runs.create(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=assistant_id,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_response(thread):\n",
        "    return client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ohA94RLEr_"
      },
      "source": [
        "I've also defined a `create_thread_and_run` function that I can re-use (which is actually almost identical to the [`client.beta.threads.create_and_run`](https://platform.openai.com/docs/api-reference/runs/createThreadAndRun) compound function in our API ;) ). Finally, we can submit our mock user requests each to a new Thread.\n",
        "\n",
        "Notice how all of these API calls are asynchronous operations; this means we actually get async behavior in our code without the use of async libraries! (e.g. `asyncio`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAO3RBlMLEr_"
      },
      "outputs": [],
      "source": [
        "def create_thread_and_run(user_input):\n",
        "    thread = client.beta.threads.create()\n",
        "    run = submit_message(MATH_ASSISTANT_ID, thread, user_input)\n",
        "    return thread, run\n",
        "\n",
        "\n",
        "# Emulating concurrent user requests\n",
        "thread1, run1 = create_thread_and_run(\n",
        "    \"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
        ")\n",
        "thread2, run2 = create_thread_and_run(\"Could you explain linear algebra to me?\")\n",
        "thread3, run3 = create_thread_and_run(\"I don't like math. What can I do?\")\n",
        "\n",
        "# Now all Runs are executing..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GldliSzMLEr_"
      },
      "source": [
        "Once all Runs are going, we can wait on each and get the responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMe3VWUkLEr_",
        "outputId": "06f362d0-9f74-4fff-c687-41c90c345769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Messages\n",
            "user: I need to solve the equation `3x + 11 = 14`. Can you help me?\n",
            "assistant: Yes, subtract 11 from both sides to get `3x = 3`, then divide both sides by 3 to find `x = 1`.\n",
            "\n",
            "# Messages\n",
            "user: Could you explain linear algebra to me?\n",
            "assistant: Linear algebra is the branch of mathematics that deals with vector spaces, linear equations, and matrices, focusing on the properties and operations that can be applied to vectors and linear transformations.\n",
            "\n",
            "# Messages\n",
            "user: I don't like math. What can I do?\n",
            "assistant: Try finding aspects of math that relate to your interests or daily life, and consider a tutor or interactive resources to make learning more enjoyable.\n",
            "\n",
            "# Messages\n",
            "user: I don't like math. What can I do?\n",
            "assistant: Try finding aspects of math that relate to your interests or daily life, and consider a tutor or interactive resources to make learning more enjoyable.\n",
            "user: Thank you!\n",
            "assistant: You're welcome! If you have any more questions, feel free to ask.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Pretty printing helper\n",
        "def pretty_print(messages):\n",
        "    print(\"# Messages\")\n",
        "    for m in messages:\n",
        "        print(f\"{m.role}: {m.content[0].text.value}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Waiting in a loop\n",
        "def wait_on_run(run, thread):\n",
        "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread.id,\n",
        "            run_id=run.id,\n",
        "        )\n",
        "        time.sleep(0.5)\n",
        "    return run\n",
        "\n",
        "\n",
        "# Wait for Run 1\n",
        "run1 = wait_on_run(run1, thread1)\n",
        "pretty_print(get_response(thread1))\n",
        "\n",
        "# Wait for Run 2\n",
        "run2 = wait_on_run(run2, thread2)\n",
        "pretty_print(get_response(thread2))\n",
        "\n",
        "# Wait for Run 3\n",
        "run3 = wait_on_run(run3, thread3)\n",
        "pretty_print(get_response(thread3))\n",
        "\n",
        "# Thank our assistant on Thread 3 :)\n",
        "run4 = submit_message(MATH_ASSISTANT_ID, thread3, \"Thank you!\")\n",
        "run4 = wait_on_run(run4, thread3)\n",
        "pretty_print(get_response(thread3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak6u5mO0LEsA"
      },
      "source": [
        "Et voilÃ !\n",
        "\n",
        "You may have noticed that this code is not actually specific to our math Assistant at all... this code will work for any new Assistant you create simply by changing the Assistant ID! That is the power of the Assistants API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BKGe-n9LEsA"
      },
      "source": [
        "## Tools\n",
        "\n",
        "A key feature of the Assistants API is the ability to equip our Assistants with Tools, like Code Interpreter, Retrieval, and custom Functions. Let's take a look at Retreival."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g910QtOgLEsB"
      },
      "source": [
        "### Retrieval\n",
        "\n",
        "Another powerful tool in the Assistants API is [Retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval): the ability to upload files that the Assistant will use as a knowledge base when answering questions. This can also be enabled from the Dashboard or the API, where we can upload files we want to be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d2WUCCELEsB"
      },
      "source": [
        "![Enabling retrieval](https://github.com/openai/openai-cookbook/blob/main/images/assistants_overview_enable_retrieval.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMlxLANhLEsB",
        "outputId": "2c416b46-63d6-4192-d56b-ea7ebf7e66a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f3ea4ca592db>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Upload the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m file = client.files.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m     file=open(\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"data/language_models_are_unsupervised_multitask_learners.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
          ]
        }
      ],
      "source": [
        "# Upload the file\n",
        "file = client.files.create(\n",
        "    file=open(\n",
        "        \"data/language_models_are_unsupervised_multitask_learners.pdf\",\n",
        "        \"rb\",\n",
        "    ),\n",
        "    purpose=\"assistants\",\n",
        ")\n",
        "# Update Assistant\n",
        "assistant = client.beta.assistants.update(\n",
        "    MATH_ASSISTANT_ID,\n",
        "    tools=[{\"type\": \"retrieval\"}],\n",
        "    file_ids=[file.id],\n",
        ")\n",
        "show_json(assistant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3GEmQ60LEsB",
        "outputId": "7910a4d9-7e70-475b-b889-a61afa4f0772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Messages\n",
            "user: What are some cool math concepts behind this ML paper pdf? Explain in two sentences.\n",
            "assistant: I am unable to find specific sections referring to \"cool math concepts\" directly in the paper using the available tools. I will now read the beginning of the paper to identify any mathematical concepts that are fundamental to the paper.\n",
            "assistant: The paper discusses leveraging large language models as a framework for unsupervised multitask learning, where tasks are implicitly defined by the context within sequences of text. It explores the zero-shot learning capabilities of such models by showing that when a language model is trained on a vast dataset, it begins to generalize and perform tasks without explicit supervision, achieving competitive results across various natural language processing tasks using a probabilistic framework based on sequential modeling and conditional probabilities.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "thread, run = create_thread_and_run(\n",
        "    \"What are some cool math concepts behind this ML paper pdf? Explain in two sentences.\"\n",
        ")\n",
        "run = wait_on_run(run, thread)\n",
        "pretty_print(get_response(thread))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYat_3PwLEsB"
      },
      "source": [
        "> **Note**\n",
        "> There are more intricacies in Retrieval, like [Annotations](https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages), which may be covered in another cookbook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itFlDzIxLEsD"
      },
      "source": [
        "Woohoo ðŸŽ‰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnNA-tGxLEsD"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We covered a lot of ground in this notebook, give yourself a high-five! Hopefully you should now have a strong foundation to build powerful, stateful experiences with tools like Code Interpreter, Retrieval, and Functions!\n",
        "\n",
        "There's a few sections we didn't cover for the sake of brevity, so here's a few resources to explore further:\n",
        "\n",
        "- [Annotations](https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages): parsing file citations\n",
        "- [Files](https://platform.openai.com/docs/api-reference/assistants/file-object): Thread scoped vs Assistant scoped\n",
        "- [Parallel Function Calls](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling): calling multiple tools in a single Step\n",
        "- Multi-Assistant Thread Runs: single Thread with Messages from multiple Assistants\n",
        "- Streaming: coming soon!\n",
        "\n",
        "Now go off and build something ama[zing](https://www.youtube.com/watch?v=xvFZjo5PgG0&pp=ygUQcmljayByb2xsIG5vIGFkcw%3D%3D)!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}